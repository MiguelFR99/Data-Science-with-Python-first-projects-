{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3353dd23-11d4-4d7c-a101-7b0f341d996c",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "## Predicting bankruptcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9d411-9a7f-409b-9151-87ba50535499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from ipywidgets import interact\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from teaching_tools.widgets import ConfusionMatrixWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d9b7d-9116-45d3-8162-506cda868941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data file\n",
    "with gzip.open(\"data/taiwan-bankruptcy-data.json.gz\", \"r\") as f:\n",
    "        taiwan_data = json.load(f)\n",
    "\n",
    "print(type(taiwan_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d3ca7a-5431-42d5-a137-de12372b3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key names\n",
    "taiwan_data_keys = taiwan_data.keys()\n",
    "print(taiwan_data_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e648ac-3587-42b8-a4c4-551c5de62a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many companies there are\n",
    "n_companies = len(taiwan_data[\"observations\"])\n",
    "print(n_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a30d9-fa39-420f-bba1-ac27f42b16ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the number of features\n",
    "n_features = len(taiwan_data[\"observations\"][0])\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9715fc-b4de-4720-9cc2-d751926cc5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle function to clean and preprocess the data\n",
    "def wrangle(filename):\n",
    "    \n",
    "    # Open compressed file, load into dictionary\n",
    "    with gzip.open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    # Load dictionary into DataFrame, set index\n",
    "    df = pd.DataFrame().from_dict(data[\"observations\"]).set_index(\"id\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc1d7f-e7df-4fd3-aa3f-91d7e01ddb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle(\"data/taiwan-bankruptcy-data.json.gz\")\n",
    "print(\"df shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6d6207-b5d8-4e44-a9ef-225beaaf5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the NaNs by column\n",
    "nans_by_col = df.isnull().sum()\n",
    "print(\"nans_by_col shape:\", nans_by_col.shape)\n",
    "nans_by_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e02962-7950-4b13-a7cc-da080c50e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if the classes are balanced\n",
    "df[\"bankrupt\"].value_counts(normalize=True).plot(kind=\"bar\", \n",
    "                                                xlabel=\"Bankrupt\", \n",
    "                                                ylabel=\"Frequency\",\n",
    "                                                title=\"Class Balance\"\n",
    "                                               )\n",
    "# As it can be seen, the class \"False\" (companies that are not in bankruptcy) are far more than the ones \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f9e60c-b7ea-47e1-9c2d-6523a65af05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical split\n",
    "target = \"bankrupt\"\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a3d9d-cc9d-41d6-b3be-8363c71809bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 42)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c5178-c4ee-4442-9f8a-4aaba75844fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a random oversampler to have a better data to work with\n",
    "over_sampler = RandomOverSampler(random_state=42)\n",
    "X_train_over, y_train_over = over_sampler.fit_resample(X_train, y_train)\n",
    "print(\"X_train_over shape:\", X_train_over.shape)\n",
    "X_train_over.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4429e-85cd-4ddc-9fb9-0cb834cda956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a random forest model with the oversampled data\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97749750-7bf6-42e5-9667-6abf34aa11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's perform a cross validation (kfolds) to better check the accuracy of the model\n",
    "cv_scores = cross_val_score(clf, X_train_over, y_train_over, cv=5, n_jobs=-1)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79736ea3-7d91-4180-aebd-223c7f781f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some hyperparameter tuning with the estimators and the max_depth of the forest\n",
    "params = {\n",
    "    \"n_estimators\": range(25, 100, 25),\n",
    "    \"max_depth\": range(10, 50, 10) \n",
    "}\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63a2e8-2b62-40e0-bd95-21f8cb9dfd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will create a GridSearch to include the classifier and the hyperparameter grid\n",
    "model = GridSearchCV(\n",
    "    clf,\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851c005-f1a8-4010-af8a-57e920943356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit the model with the oversampled data\n",
    "model.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b02add-c694-4354-9b8d-ff1d482cffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put the results of the model in a DataFrame, to see which set of hyperparameters performs better\n",
    "cv_results = pd.DataFrame(model.cv_results_).sort_values(by=\"rank_test_score\")\n",
    "cv_results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b5448-eebb-4e19-af2a-d6f89a5d947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I choose the best set of hyperparameters\n",
    "best_params = model.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd2e99-ff94-400b-a81c-1309da7e1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check the accuracy of the model comparing it to the test data\n",
    "acc_train = model.score(X_train, y_train)\n",
    "acc_test = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Model Training Accuracy:\", round(acc_train, 4))\n",
    "print(\"Model Test Accuracy:\", round(acc_test, 4))\n",
    "# As we can see, the accuracy of the test model is of 97,64%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddd544-db04-49e5-aa45-644ad93c00bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cb3d12-ebab-4ef4-b6fc-ee55a3545103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, it is normal that with a data that is so imbalanced we can have that accuracy by just predicting that all\n",
    "# the companies are False (no bankruptcy). However, depending on the use the model will have, we will be interested\n",
    "# in different scores (mainly precision or recall). The model not only beats the baseline, but also let the user\n",
    "# choose which scores he prefers depending on the case of use\n",
    "class_report = classification_report(y_test, model.predict(X_test))\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715d232-875d-4372-9f9b-7f6f4fb3e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the importances and plot them sorted\n",
    "features = X_train_over.columns\n",
    "importances = model.best_estimator_.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=features).sort_values()\n",
    "feat_imp.tail(10).plot(kind=\"barh\")\n",
    "plt.xlabel(\"Gini Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e288e3a-4e45-41bc-b89d-f0f6390f2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At last, I will save the model\n",
    "# Save model\n",
    "with open(\"model-5-5.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54dee1b-4e5b-4030-a4c6-7d9ca1c82243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I want to predict another dataset similar to the one that I had used, I could import the model and use it\n",
    "from my_predictor_assignment import make_predictions\n",
    "\n",
    "# Generate predictions\n",
    "y_test_pred = make_predictions(\n",
    "    data_filepath=\"data/taiwan-bankruptcy-data-test-features.json.gz\",\n",
    "    model_filepath=\"model-5-5.pkl\",\n",
    ")\n",
    "\n",
    "print(\"predictions shape:\", y_test_pred.shape)\n",
    "y_test_pred.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
