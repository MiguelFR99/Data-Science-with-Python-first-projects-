{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35f69e9-3315-486c-80e2-032f4b5ffe7e",
   "metadata": {},
   "source": [
    "# Logistic regression and decision tree\n",
    "## Predicting the damage of an earthquake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c89d4-a08c-4b10-92e5-2315675d7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import seaborn as sns\n",
    "from category_encoders import OrdinalEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc2f3d-3f1d-4be3-9b55-f52c71bde902",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02040c6a-4b28-4f3a-82fc-f64471cee194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SQL\n",
    "%load_ext sql\n",
    "%sql sqlite:////home/jovyan/nepal.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21ccbc-e677-4fdf-8e3c-559ea1fa3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First query\n",
    "%%sql\n",
    "SELECT distinct(district_id)\n",
    "FROM id_map\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e22182-a830-4554-acf0-d3917279f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second query\n",
    "%%sql\n",
    "SELECT count(*)\n",
    "FROM id_map\n",
    "WHERE district_id=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b81b18-05ae-4153-8fdf-447ebf68db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third query\n",
    "%%sql\n",
    "SELECT count(*)\n",
    "FROM id_map\n",
    "WHERE district_id=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8943af8-2263-4445-bbfa-bc19e150c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final query merging all the exploratory queries to extract the data I need\n",
    "%%sql\n",
    "SELECT distinct(im.building_id) AS b_id, bs.*, bd.damage_grade\n",
    "FROM id_map AS im\n",
    "JOIN building_structure AS bs\n",
    "    ON im.building_id = bs.building_id\n",
    "JOIN building_damage AS bd\n",
    "    ON im.building_id = bd.building_id\n",
    "WHERE im.district_id = 3\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c78c8b-1523-4753-bf0b-6945e5fc218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start cleaning the data, I will check the correlation between the variables, except the independent \"severe_damage\"\n",
    "correlation = df.select_dtypes(\"number\").drop(columns=\"severe_damage\").corr()\n",
    "sns.heatmap(correlation);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f5232-596b-47e5-980d-4f1175824c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therea are two variables highly correlated between them\n",
    "print(df[\"severe_damage\"].corr(df[\"height_ft_pre_eq\"]))\n",
    "print(df[\"severe_damage\"].corr(df[\"count_floors_pre_eq\"]))\n",
    "# As the correlation of the second one with the independent variable is closer to 0, it has less predictive power and I'll drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793aac53-047a-4739-b3f1-342ffc585535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I built a wrangle function that will return a dataframe cleaned from the data extracted\n",
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Construct query\n",
    "    query = \"\"\"\n",
    "            SELECT distinct(im.building_id) AS b_id, bs.*, bd.damage_grade\n",
    "            FROM id_map AS im\n",
    "            JOIN building_structure AS bs\n",
    "                ON im.building_id = bs.building_id\n",
    "            JOIN building_damage AS bd\n",
    "                ON im.building_id = bd.building_id\n",
    "            WHERE im.district_id = 3\n",
    "            \"\"\"\n",
    "            \n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col=\"b_id\")\n",
    "\n",
    "    # Identify leaky columns\n",
    "    drop_cols = [col for col in df.columns if \"post_eq\" in col]\n",
    "\n",
    "    # Add high-cardinality / redundant column\n",
    "    drop_cols.append(\"building_id\")\n",
    "\n",
    "    # Create binary target column\n",
    "    df[\"damage_grade\"] = df[\"damage_grade\"].str[-1].astype(int)\n",
    "    df[\"severe_damage\"] = (df[\"damage_grade\"] > 3).astype(int)\n",
    "\n",
    "    # Drop old target\n",
    "    drop_cols.append(\"damage_grade\")\n",
    "    \n",
    "    # Drop multicollinearity column\n",
    "    drop_cols.append(\"count_floors_pre_eq\")\n",
    "    \n",
    "    # Drop columns\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb12e55-fa11-45af-bb04-3093e6dbbe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67fe8c6-2f10-4f2f-b752-d688e2c504c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the class balance of the independent variable\n",
    "df[\"severe_damage\"].value_counts(normalize=True).plot(\n",
    "    kind=\"bar\", xlabel=\"Class\", ylabel=\"Relative Frequency\", title=\"Class Balance\"\n",
    ");\n",
    "# As it can be seen, the classes are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd86fd8-dc95-46b7-a357-42f0fa878488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if there is any relationship between footprint size of a building and the damage it sustained in the earthquake\n",
    "sns.boxplot(x=\"severe_damage\", y=\"plinth_area_sq_ft\", data=df)\n",
    "plt.xlabel(\"Severe Damage\")\n",
    "plt.ylabel(\"Plinth Area [sq. ft.]\")\n",
    "plt.title(\"Karepalanchok, Plinth Area vs Building Damage\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48289ff0-3570-40ca-9c93-64b89cbc1279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will check if the buildings from a certain type are more propense to have severe_damage\n",
    "roof_pivot = pd.pivot_table(\n",
    "    df, index=\"roof_type\", values=\"severe_damage\", aggfunc=np.mean\n",
    ").sort_values(by=\"severe_damage\")\n",
    "roof_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1fc27-2ad6-41a9-bc47-8d26779e997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once I have finished the \n",
    "target = \"severe_damage\"\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d25c3-8e34-417d-a416-b4948be2e5f8",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93768d44-0914-4a06-81ad-0cfdd50d3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the vertical split\n",
    "target = \"severe_damage\"\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a699488-88b7-4417-9715-212eec3ee659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the vertical split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61586cdf-04a0-4f9c-8963-1aa0578d73eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_baseline = y_train.value_counts(normalize=True).max()\n",
    "print(\"Baseline Accuracy:\", round(acc_baseline, 2))\n",
    "# The accuracy of the Baseline is 0.55, so let's try to beat it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09f582-4348-493a-ae80-c9f0c0bd3484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will create a model with a logistic regression for beating the baseline\n",
    "model_lr = model = make_pipeline(\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2901c0-21b0-4196-9b6b-9b5dd873fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will check the accuracy of the model with the validation set\n",
    "lr_train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "lr_val_acc = model.score(X_val, y_val)\n",
    "\n",
    "print(\"Logistic Regression, Training Accuracy Score:\", lr_train_acc)\n",
    "print(\"Logistic Regression, Validation Accuracy Score:\", lr_val_acc)\n",
    "# The accuracy of both is around 65%, but it could be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9539a14-5d8f-40a9-b54c-16174e2c0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the accuracy score could be better, I will try to change the model to a decision tree\n",
    "# In addition, I will try to check which are the best hyperparameters for the model\n",
    "depth_hyperparams = range(1, 16)\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "for d in depth_hyperparams:\n",
    "    model_dt = make_pipeline(\n",
    "    OrdinalEncoder(), DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    ")\n",
    "    model_dt.fit(X_train, y_train)\n",
    "    training_acc.append(model_dt.score(X_train, y_train))\n",
    "    # Calculate validation accuracy score and append to `training_acc`\n",
    "    validation_acc.append(model_dt.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5222e81-ca79-4734-919b-3a386a252ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the results, so that we can get the better hyperparameters\n",
    "plt.plot(depth_hyperparams, training_acc, label=\"training\")\n",
    "plt.plot(depth_hyperparams, validation_acc, label=\"validation\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69360572-d6a4-4ae1-89b1-3e88d21ec059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final model will have a max_depth of 10\n",
    "final_model_dt = make_pipeline(\n",
    "    OrdinalEncoder(), DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    ")\n",
    "final_model_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b157c-c3d8-4234-8c18-f80ac2562929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's read the test to see if the model performs well\n",
    "X_test = pd.read_csv(\"data/kavrepalanchok-test-features.csv\", index_col=\"b_id\")\n",
    "y_test_pred = final_model_dt.predict(X_test)\n",
    "y_test_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1678184f-ecac-41c8-83a2-132e44fc9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the importances of the features \n",
    "features = X_train.columns\n",
    "importances = final_model_dt.named_steps[\"decisiontreeclassifier\"].feature_importances_\n",
    "feat_imp = pd.Series(importances, index=features).sort_values()\n",
    "feat_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a528b55f-870f-4d28-83ba-606377dd5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I represent them in an horizontal bar chart\n",
    "# Create horizontal bar chart of feature importances\n",
    "feat_imp.plot(kind=\"barh\")\n",
    "plt.xlabel(\"Gini Importance\")\n",
    "plt.ylabel(\"Feature\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9aa7d-40ca-44ac-bf2b-349496ef6b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As it can be seen, the feature roof_type is the most important feature (by far) affecting severe damage after an earthquake"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
